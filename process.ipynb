{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "\n",
    "# Requirements\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import fiona\n",
    "\n",
    "\n",
    "# Parameters\n",
    "data_foldername = 'data'\n",
    "data_directory = pathlib.Path('.').absolute() / data_foldername\n",
    "data_directory = str(data_directory)\n",
    "data_directory = data_directory + '\\\\'\n",
    "\n",
    "gpx_foldername = 'gpxfiles'\n",
    "gpx_directory = pathlib.Path('.').absolute() / gpx_foldername\n",
    "\n",
    "#new_gpx_foldername = 'new_gpxfiles'\n",
    "#gpx_directory = pathlib.Path('.').absolute() / new_gpx_foldername \n",
    "\n",
    "#processed_gpx_foldername = 'processed_gpxfiles'\n",
    "#processed_gpx_directory = pathlib.Path('.').absolute() / processed_gpx_foldername \n",
    "\n",
    "# List of GeoPackage files to read\n",
    "gpkg_files = os.listdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 0 (country-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_0')\n",
    "\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm0 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm0 = gdf_adm0.reset_index(drop=True)\n",
    "\n",
    "#gdf_adm0.explore()\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm0, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "#gdf_runs_adm0 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm0, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_0'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm0 = gpd.GeoDataFrame(pd.concat(gdf_adminregions_per_gpx, ignore_index=True))\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm0 = gdf_runs_adm0.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_0',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "#col_list = ['GID_0', 'COUNTRY', 'geometry']\n",
    "col_list = ['COUNTRY', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm0 = gdf_runs_adm0[col_list]\n",
    "gdf_runs_adm0 = gdf_runs_adm0.rename(columns={\"COUNTRY\": \"NAME\"})\n",
    "gdf_runs_adm0.columns = gdf_runs_adm0.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm0 = gdf_runs_adm0.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm0['geometry'] = gdf_runs_adm0['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_0.geojson\")):\n",
    "    os.remove(\"adm_0.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm0.to_file(\"adm_0.geojson\", driver='GeoJSON')\n",
    "\n",
    "#gdf_runs_adm0.explore()\n",
    "#gdf_runs_adm0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 1 (province-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_1')\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm1 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "gdf_adm1 = gdf_adm1[~gdf_adm1['ENGTYPE_1'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm1 = gdf_adm1.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "\n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm1, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm1, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_1'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm1 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm1 = gdf_runs_adm1.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_1',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_1', 'TYPE_1', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm1 = gdf_runs_adm1[col_list]\n",
    "gdf_runs_adm1 = gdf_runs_adm1.rename(columns={\"NAME_1\": \"NAME\"})\n",
    "gdf_runs_adm1 = gdf_runs_adm1.rename(columns={\"TYPE_1\": \"TYPE\"})\n",
    "gdf_runs_adm1.columns = gdf_runs_adm1.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm1 = gdf_runs_adm1.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm1['geometry'] = gdf_runs_adm1['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_1.geojson\")):\n",
    "    os.remove(\"adm_1.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm1.to_file(\"adm_1.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 2 (municipality-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_2')\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm2 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "gdf_adm2 = gdf_adm2[~gdf_adm2['ENGTYPE_2'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm2 = gdf_adm2.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm2, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm2, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_2'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm2 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm2 = gdf_runs_adm2.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_2',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_2', 'TYPE_2', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm2 = gdf_runs_adm2[col_list]\n",
    "gdf_runs_adm2 = gdf_runs_adm2.rename(columns={\"NAME_2\": \"NAME\"})\n",
    "gdf_runs_adm2 = gdf_runs_adm2.rename(columns={\"TYPE_2\": \"TYPE\"})\n",
    "gdf_runs_adm2.columns = gdf_runs_adm2.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm2 = gdf_runs_adm2.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm2['geometry'] = gdf_runs_adm2['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_2.geojson\")):\n",
    "    os.remove(\"adm_2.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm2.to_file(\"adm_2.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 'ADM_ADM_3' read from 'gadm41_AUT.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_BEL.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_CHE.gpkg'.\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_CZE.gpkg' (fallback).\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_DEU.gpkg'.\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_DNK.gpkg' (fallback).\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_ESP.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_FRA.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_ITA.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_LUX.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_MWI.gpkg'.\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_NLD.gpkg' (fallback).\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_POL.gpkg'.\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_SVK.gpkg' (fallback).\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_SVN.gpkg' (fallback).\n",
      "Layer 'ADM_ADM_2' read from 'gadm41_SWE.gpkg' (fallback).\n",
      "Combined GeoDataFrame created.\n",
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 3 (district-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Specify the layer names you want to check\n",
    "ADM_ADM_3 = 'ADM_ADM_3'\n",
    "ADM_ADM_2 = 'ADM_ADM_2'\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    file_path = os.path.join(data_directory, gpkg_file)\n",
    "    try:\n",
    "        # Try to read ADM3 layer\n",
    "        gdf = gpd.read_file(file_path, layer=ADM_ADM_3)\n",
    "        gdf = gdf[~gdf['ENGTYPE_3'].str.contains(\"Water body\")]\n",
    "        gdf_list.append(gdf)\n",
    "        print(f\"Layer '{ADM_ADM_3}' read from '{gpkg_file}'.\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            # Fall back to ADM2 layer\n",
    "            gdf = gpd.read_file(file_path, layer=ADM_ADM_2)\n",
    "            gdf = gdf[~gdf['ENGTYPE_2'].str.contains(\"Water body\")]\n",
    "            gdf_list.append(gdf)\n",
    "            print(f\"Layer '{ADM_ADM_2}' read from '{gpkg_file}' (fallback).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read either '{ADM_ADM_3}' or '{ADM_ADM_2}' from '{gpkg_file}': {e}\")\n",
    "\n",
    "# Concatenate the GeoDataFrames if any were found\n",
    "if gdf_list:\n",
    "    gdf_adm3 = pd.concat(gdf_list, ignore_index=True)\n",
    "    print(\"Combined GeoDataFrame created.\")\n",
    "else:\n",
    "    print(f\"No GeoPackages contained the layer '{ADM_ADM_3}'.\")\n",
    "\n",
    "#gdf_adm3 = gdf_adm3[~gdf_adm3['ENGTYPE_3'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm3 = gdf_adm3.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm3, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm3, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_3'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm3 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm3 = gdf_runs_adm3.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_3',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_3', 'TYPE_3', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm3 = gdf_runs_adm3[col_list]\n",
    "gdf_runs_adm3 = gdf_runs_adm3.rename(columns={\"NAME_3\": \"NAME\"})\n",
    "gdf_runs_adm3 = gdf_runs_adm3.rename(columns={\"TYPE_3\": \"TYPE\"})\n",
    "gdf_runs_adm3.columns = gdf_runs_adm3.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm3 = gdf_runs_adm3.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm3['geometry'] = gdf_runs_adm3['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_3.geojson\")):\n",
    "    os.remove(\"adm_3.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm3.to_file(\"adm_3.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of provinces per country\n",
    "\n",
    "# Set your subfolder name here\n",
    "subfolder_name = \"data\"\n",
    "folder_path = os.path.join(os.getcwd(), subfolder_name)\n",
    "\n",
    "# Dictionary to hold unique name1 values per country\n",
    "provinces_counts = defaultdict(set)\n",
    "\n",
    "# Iterate through each file in the subfolder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.gpkg'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # List all layers in the GeoPackage\n",
    "            layers = fiona.listlayers(file_path)\n",
    "            if 'ADM_ADM_1' not in layers:\n",
    "                print(f\"Layer 'adm_1' not found in {filename}, skipping...\")\n",
    "                continue\n",
    "            # Read the adm_1 layer\n",
    "            gdf = gpd.read_file(file_path, layer='ADM_ADM_1')\n",
    "            # Filter out rows where type1 is 'Water body'\n",
    "            gdf_filtered = gdf[gdf['TYPE_1'] != 'Water body']\n",
    "            # Group by country and collect unique name1 values\n",
    "            for country, group in gdf_filtered.groupby('COUNTRY'):\n",
    "                unique_name1s = group['NAME_1'].unique()\n",
    "                provinces_counts[country].update(unique_name1s)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_provinces = pd.DataFrame(\n",
    "    [(country, len(name1_set)) for country, name1_set in provinces_counts.items()],\n",
    "    columns=['country', 'provinces_total']\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(df_provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of municipalities per country\n",
    "\n",
    "# Dictionary to hold unique name1 values per country\n",
    "municipalities_counts = defaultdict(set)\n",
    "\n",
    "# Iterate through each file in the subfolder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.gpkg'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # List all layers in the GeoPackage\n",
    "            layers = fiona.listlayers(file_path)\n",
    "            if 'ADM_ADM_2' not in layers:\n",
    "                print(f\"Layer 'adm_1' not found in {filename}, skipping...\")\n",
    "                continue\n",
    "            # Read the adm_1 layer\n",
    "            gdf = gpd.read_file(file_path, layer='ADM_ADM_2')\n",
    "            # Filter out rows where type1 is 'Water body'\n",
    "            gdf_filtered = gdf[gdf['TYPE_2'] != 'Water body']\n",
    "            # Group by country and collect unique name1 values\n",
    "            for country, group in gdf_filtered.groupby('COUNTRY'):\n",
    "                # Create a combined key for uniqueness\n",
    "                group['combined_key'] = group['NAME_1'].astype(str) + \"_\" + group['NAME_2'].astype(str)\n",
    "                unique_combinations = group['combined_key'].unique()\n",
    "                municipalities_counts[country].update(unique_combinations)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_municipalities = pd.DataFrame(\n",
    "    [(country, len(name1_set)) for country, name1_set in municipalities_counts.items()],\n",
    "    columns=['country', 'muncipalities_total']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 'adm_1' not found in gadm41_CZE.gpkg, skipping...\n",
      "Layer 'adm_1' not found in gadm41_DNK.gpkg, skipping...\n",
      "Layer 'adm_1' not found in gadm41_NLD.gpkg, skipping...\n",
      "Layer 'adm_1' not found in gadm41_SVK.gpkg, skipping...\n",
      "Layer 'adm_1' not found in gadm41_SVN.gpkg, skipping...\n",
      "Layer 'adm_1' not found in gadm41_SWE.gpkg, skipping...\n"
     ]
    }
   ],
   "source": [
    "# Number of districs per country\n",
    "\n",
    "# Dictionary to hold unique name1 values per country\n",
    "districts_counts = defaultdict(set)\n",
    "\n",
    "# Iterate through each file in the subfolder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.gpkg'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # List all layers in the GeoPackage\n",
    "            layers = fiona.listlayers(file_path)\n",
    "            if 'ADM_ADM_3' not in layers:\n",
    "                print(f\"Layer 'adm_1' not found in {filename}, skipping...\")\n",
    "                continue\n",
    "            # Read the adm_1 layer\n",
    "            gdf = gpd.read_file(file_path, layer='ADM_ADM_3')\n",
    "            # Filter out rows where type1 is 'Water body'\n",
    "            gdf_filtered = gdf[gdf['TYPE_3'] != 'Water body']\n",
    "            # Group by country and collect unique name1 values\n",
    "            for country, group in gdf_filtered.groupby('COUNTRY'):\n",
    "                # Create a combined key for uniqueness\n",
    "                group['combined_key'] = group['NAME_1'].astype(str) + \"_\" + group['NAME_2'].astype(str) + \"_\" + group['NAME_3'].astype(str) \n",
    "                unique_combinations = group['combined_key'].unique()\n",
    "                districts_counts[country].update(unique_combinations)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_districts = pd.DataFrame(\n",
    "    [(country, len(name1_set)) for country, name1_set in districts_counts.items()],\n",
    "    columns=['country', 'districts_total']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge total regions with regions run\n",
    "\n",
    "provincies = gpd.read_file(\"adm_1.geojson\")\n",
    "# count number of provinces per country\n",
    "\n",
    "#2. Convert to a DataFrame while naming the count column\n",
    "count_provinces_run = (\n",
    "    provincies\n",
    "    .groupby(['country'])\n",
    "    .size()\n",
    "    .reset_index(name='provinces_run')\n",
    ")\n",
    "\n",
    "adm_1_stats = count_provinces_run.merge(df_provinces, on='country', how='inner')\n",
    "#print(adm_1_stats)\n",
    "\n",
    "#municipalities_run\n",
    "municipalities = gpd.read_file(\"adm_2.geojson\")\n",
    "# count number of provinces per country\n",
    "\n",
    "#2. Convert to a DataFrame while naming the count column\n",
    "count_municipalities_run = (\n",
    "    municipalities\n",
    "    .groupby(['country'])\n",
    "    .size()\n",
    "    .reset_index(name='municipalities_run')\n",
    ")\n",
    "\n",
    "adm_2_stats = count_municipalities_run.merge(df_municipalities, on='country', how='inner')\n",
    "\n",
    "#districts_run\n",
    "districts = gpd.read_file(\"adm_3.geojson\")\n",
    "# count number of provinces per country\n",
    "\n",
    "#2. Convert to a DataFrame while naming the count column\n",
    "count_districts_run = (\n",
    "    districts\n",
    "    .groupby(['country'])\n",
    "    .size()\n",
    "    .reset_index(name='districts_run')\n",
    ")\n",
    "\n",
    "adm_3_stats = count_districts_run.merge(df_districts, on='country', how='inner')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# WRITE OUTPUTS\n",
    "# ----------------------------------------------------------------------\n",
    "# JSON â€“ easy for programs to consume\n",
    "\n",
    "adm_1_stats.to_json('adm_1_stats.json', orient='records', lines=True)\n",
    "adm_2_stats.to_json('adm_2_stats.json', orient='records', lines=True)\n",
    "adm_3_stats.to_json('adm_3_stats.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful statements\n",
    "\n",
    "# import fiona\n",
    "# layers = fiona.listlayers(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Move data from new_gpxfiles dir to processed_gpxfiles dir\\n# gather all files in source folder\\nallfiles = os.listdir(gpx_directory)\\n \\n# iterate on all files to move them to destination folder\\nfor f in allfiles:\\n    src_path = os.path.join(gpx_directory, f)\\n    dst_path = os.path.join(processed_gpx_directory, f)\\n    os.rename(src_path, dst_path)\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Move data from new_gpxfiles dir to processed_gpxfiles dir\n",
    "# gather all files in source folder\n",
    "allfiles = os.listdir(gpx_directory)\n",
    " \n",
    "# iterate on all files to move them to destination folder\n",
    "for f in allfiles:\n",
    "    src_path = os.path.join(gpx_directory, f)\n",
    "    dst_path = os.path.join(processed_gpx_directory, f)\n",
    "    os.rename(src_path, dst_path)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
