{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "\n",
    "# TODO\n",
    "# https://stackoverflow.com/questions/58257251/how-to-check-if-a-file-exists-in-another-folder\n",
    "\n",
    "# Requirements\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Parameters\n",
    "data_foldername = 'data'\n",
    "data_directory = pathlib.Path('.').absolute() / data_foldername\n",
    "data_directory = str(data_directory)\n",
    "data_directory = data_directory + '\\\\'\n",
    "\n",
    "gpx_foldername = 'gpxfiles'\n",
    "gpx_directory = pathlib.Path('.').absolute() / gpx_foldername\n",
    "\n",
    "#new_gpx_foldername = 'new_gpxfiles'\n",
    "#gpx_directory = pathlib.Path('.').absolute() / new_gpx_foldername \n",
    "\n",
    "#processed_gpx_foldername = 'processed_gpxfiles'\n",
    "#processed_gpx_directory = pathlib.Path('.').absolute() / processed_gpx_foldername \n",
    "\n",
    "# List of GeoPackage files to read\n",
    "gpkg_files = os.listdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 0 (country-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_0')\n",
    "\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm0 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm0 = gdf_adm0.reset_index(drop=True)\n",
    "\n",
    "#gdf_adm0.explore()\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm0, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "#gdf_runs_adm0 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm0, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_0'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm0 = gpd.GeoDataFrame(pd.concat(gdf_adminregions_per_gpx, ignore_index=True))\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm0 = gdf_runs_adm0.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_0',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "#col_list = ['GID_0', 'COUNTRY', 'geometry']\n",
    "col_list = ['COUNTRY', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm0 = gdf_runs_adm0[col_list]\n",
    "gdf_runs_adm0.columns = gdf_runs_adm0.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm0 = gdf_runs_adm0.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm0['geometry'] = gdf_runs_adm0['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_0.geojson\")):\n",
    "    os.remove(\"adm_0.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm0.to_file(\"adm_0.geojson\", driver='GeoJSON')\n",
    "\n",
    "#gdf_runs_adm0.explore()\n",
    "#gdf_runs_adm0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 1 (province-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_1')\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm1 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "gdf_adm1 = gdf_adm1[~gdf_adm1['ENGTYPE_1'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm1 = gdf_adm1.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "\n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm1, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm1, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_1'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm1 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm1 = gdf_runs_adm1.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_1',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_1', 'TYPE_1', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm1 = gdf_runs_adm1[col_list]\n",
    "gdf_runs_adm1 = gdf_runs_adm1.rename(columns={\"NAME_1\": \"NAME\"})\n",
    "gdf_runs_adm1 = gdf_runs_adm1.rename(columns={\"TYPE_1\": \"TYPE\"})\n",
    "gdf_runs_adm1.columns = gdf_runs_adm1.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm1 = gdf_runs_adm1.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm1['geometry'] = gdf_runs_adm1['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_1.geojson\")):\n",
    "    os.remove(\"adm_1.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm1.to_file(\"adm_1.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 2 (municipality-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    # Read the GeoPackage file\n",
    "    gdf_file = gpd.read_file(os.path.join(data_directory,  gpkg_file), layer='ADM_ADM_2')\n",
    "    gdf_list.append(gdf_file)\n",
    "\n",
    "gdf_adm2 = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "gdf_adm2 = gdf_adm2[~gdf_adm2['ENGTYPE_2'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm2 = gdf_adm2.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm2, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm2, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_2'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm2 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm2 = gdf_runs_adm2.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_2',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_2', 'TYPE_2', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm2 = gdf_runs_adm2[col_list]\n",
    "gdf_runs_adm2 = gdf_runs_adm2.rename(columns={\"NAME_2\": \"NAME\"})\n",
    "gdf_runs_adm2 = gdf_runs_adm2.rename(columns={\"TYPE_2\": \"TYPE\"})\n",
    "gdf_runs_adm2.columns = gdf_runs_adm2.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm2 = gdf_runs_adm2.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm2['geometry'] = gdf_runs_adm2['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_2.geojson\")):\n",
    "    os.remove(\"adm_2.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm2.to_file(\"adm_2.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 'ADM_ADM_3' read from 'gadm41_AUT.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_BEL.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_CHE.gpkg'.\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_CZE.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_DEU.gpkg'.\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_DNK.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_ESP.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_FRA.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_ITA.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_LUX.gpkg'.\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_MWI.gpkg'.\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_NLD.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Layer 'ADM_ADM_3' read from 'gadm41_POL.gpkg'.\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_SVK.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_SVN.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Layer 'ADM_ADM_3' not found in 'gadm41_SWE.gpkg': Null layer: 'ADM_ADM_3'\n",
      "Combined GeoDataFrame created.\n",
      "File Deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## Proces for admin regions 3 (district-level)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "# Specify the layer name you want to check\n",
    "specific_layer = 'ADM_ADM_3'\n",
    "\n",
    "# Loop through each GeoPackage file and read the data\n",
    "for gpkg_file in gpkg_files:\n",
    "    try:\n",
    "        # Attempt to read the specific layer\n",
    "        gdf_file = gpd.read_file(os.path.join(data_directory, gpkg_file), layer=specific_layer)\n",
    "        gdf_list.append(gdf_file)\n",
    "        print(f\"Layer '{specific_layer}' read from '{gpkg_file}'.\")\n",
    "    except ValueError as e:\n",
    "        # Handle the case where the layer does not exist\n",
    "        print(f\"Layer '{specific_layer}' not found in '{gpkg_file}': {e}\")\n",
    "\n",
    "# Concatenate the GeoDataFrames if any were found\n",
    "if gdf_list:\n",
    "    gdf_adm3 = pd.concat(gdf_list, ignore_index=True)\n",
    "    print(\"Combined GeoDataFrame created.\")\n",
    "else:\n",
    "    print(f\"No GeoPackages contained the layer '{specific_layer}'.\")\n",
    "\n",
    "gdf_adm3 = gdf_adm3[~gdf_adm3['ENGTYPE_3'].str.contains(\"Water body\")]\n",
    "\n",
    "# Reset the index of the merged GeoDataFrame\n",
    "gdf_adm3 = gdf_adm3.reset_index(drop=True)\n",
    "\n",
    "# Initialize an empty list to store GeoDataFrames\n",
    "gdf_adminregions_per_gpx = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "#for filename in os.listdir(gpx_directory):\n",
    "#    if filename.endswith('.gpx'):\n",
    "#        gpxfile = os.path.join(gpx_directory, filename)\n",
    "#        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "#        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        # Perform the intersection\n",
    "#        gdf_join = gpd.sjoin(left_df=gdf_adm3, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "#        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Initialize a DataFrame to keep track of stats\n",
    "admin_region_stats = pd.DataFrame(columns=['admin_id', 'count', 'first_date', 'last_date'])\n",
    "\n",
    "for filename in os.listdir(gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(gpx_directory, filename)\n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        run_date = date\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=gdf_adm3, right_df=gdf_gpxline, how=\"inner\", predicate=\"intersects\")\n",
    "        # Get unique admin regions in this intersection\n",
    "        admin_ids = gdf_join['GID_3'].unique()\n",
    "\n",
    "        # Update stats for each admin region\n",
    "        for admin_id in admin_ids:\n",
    "            if admin_id in admin_region_stats['admin_id'].values:\n",
    "                # Update count\n",
    "                admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'count'] += 1\n",
    "                # Update first_date if current run_date is earlier or if first_date is None\n",
    "                if run_date:\n",
    "                    current_first_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'].iloc[0]\n",
    "                    if current_first_date is None or run_date < current_first_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'first_date'] = run_date\n",
    "                    # Update last_date if current run_date is later or if last_date is None\n",
    "                    current_last_date = admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'].iloc[0]\n",
    "                    if current_last_date is None or run_date > current_last_date:\n",
    "                        admin_region_stats.loc[admin_region_stats['admin_id'] == admin_id, 'last_date'] = run_date\n",
    "            else:\n",
    "                # Add new entry\n",
    "                admin_region_stats.loc[len(admin_region_stats)] = [admin_id, 1, run_date, run_date]\n",
    "\n",
    "        # Append data to temp gdf\n",
    "        gdf_adminregions_per_gpx.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_runs_adm3 = gpd.GeoDataFrame( pd.concat(gdf_adminregions_per_gpx, ignore_index=True) )\n",
    "\n",
    "# Merge stats back into the main GeoDataFrame\n",
    "gdf_runs_adm3 = gdf_runs_adm3.merge(\n",
    "    admin_region_stats,\n",
    "    left_on='GID_3',\n",
    "    right_on='admin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select columns\n",
    "col_list = ['COUNTRY', 'NAME_3', 'TYPE_3', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_runs_adm3 = gdf_runs_adm3[col_list]\n",
    "gdf_runs_adm3 = gdf_runs_adm3.rename(columns={\"NAME_3\": \"NAME\"})\n",
    "gdf_runs_adm3 = gdf_runs_adm3.rename(columns={\"TYPE_3\": \"TYPE\"})\n",
    "gdf_runs_adm3.columns = gdf_runs_adm3.columns.str.lower()\n",
    "\n",
    "gdf_runs_adm3 = gdf_runs_adm3.drop_duplicates([\"geometry\"])\n",
    "\n",
    "# Simplify geometries with a tolerance (in units of the CRS)\n",
    "# The higher the tolerance, the more simplified the geometry\n",
    "tolerance = 0.001  # Adjust as needed\n",
    "gdf_runs_adm3['geometry'] = gdf_runs_adm3['geometry'].simplify(tolerance)\n",
    "\n",
    "# Write geojsonfile\n",
    "if(os.path.isfile(\"adm_3.geojson\")):\n",
    "    os.remove(\"adm_3.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "gdf_runs_adm3.to_file(\"adm_3.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful statements\n",
    "\n",
    "# import fiona\n",
    "# layers = fiona.listlayers(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Move data from new_gpxfiles dir to processed_gpxfiles dir\\n# gather all files in source folder\\nallfiles = os.listdir(gpx_directory)\\n \\n# iterate on all files to move them to destination folder\\nfor f in allfiles:\\n    src_path = os.path.join(gpx_directory, f)\\n    dst_path = os.path.join(processed_gpx_directory, f)\\n    os.rename(src_path, dst_path)\\n    '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Move data from new_gpxfiles dir to processed_gpxfiles dir\n",
    "# gather all files in source folder\n",
    "allfiles = os.listdir(gpx_directory)\n",
    " \n",
    "# iterate on all files to move them to destination folder\n",
    "for f in allfiles:\n",
    "    src_path = os.path.join(gpx_directory, f)\n",
    "    dst_path = os.path.join(processed_gpx_directory, f)\n",
    "    os.rename(src_path, dst_path)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
